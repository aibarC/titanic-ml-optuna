# ğŸš¢ Titanic Survival Prediction â€” End-to-End ML + Streamlit + Docker

An ML project that predicts Titanic passenger survival with a **full reproducible pipeline**:

EDA â†’ Feature Engineering â†’ Statistical feature checks â†’ Optuna tuning (XGBoost) â†’ **custom end-to-end pipeline (raw â†’ processed â†’ model)** â†’ Streamlit app â†’ Docker.

---

## ğŸ”— Live Demo
**Streamlit app:** https://titanic-ml-optuna.streamlit.app/

---

## âœ¨ Highlights
- âœ… Feature engineering + statistical validation:
  - **Categorical:** chi-square, survival-rate comparison
  - **Numerical:** Mannâ€“Whitney U, Welchâ€™s t-test, Cohenâ€™s d
- âœ… Correlation filtering to reduce duplicated information:
  - Pearson / Spearman (numeric), CramÃ©râ€™s V (categorical)
- âœ… Feature selection:
  - greedy selection + L1 (Lasso) + permutation importance
- âœ… Optuna hyperparameter tuning + saving best params and **optimal F1 threshold**
- âœ… Sanity check: **shuffle target â†’ ROC-AUC â‰ˆ 0.5** (leakage / â€œlucky splitâ€ check)
- âœ… Deployment:
  - Streamlit app + Docker + Docker Compose

---

## ğŸ“Œ Table of Contents
- [Project Overview](#-project-overview)
- [Repository Structure](#-repository-structure)
- [Results](#-results)
- [How It Works (Step-by-Step)](#-how-it-works-step-by-step)
- [Run Locally](#ï¸-run-locally)
- [Run with Docker](#-run-with-docker)
- [Artifacts](#-artifacts)
- [Roadmap](#-roadmap)
- [License](#-license)

---

## ğŸ“– Project Overview
**Goal:** build an end-to-end, reproducible ML pipeline to predict `Survived (0/1)`, covering the full path:
raw data â†’ processing â†’ training â†’ artifact saving â†’ inference â†’ UI.

**Modeling approach:**
- Baseline for feature validation: **Logistic Regression**
- Final model: **XGBoost** (good speed/quality trade-off)

**Primary metric during feature work:** **ROC-AUC**  
**Final comparison metrics:** Accuracy / Precision / Recall / F1

**Evaluation setup:**
- Holdout split: `train_test_split(..., stratify=y, random_state=...)`
- Cross-validation: `StratifiedKFold` used with `cross_val_predict` to obtain out-of-fold predictions


---

## ğŸ§± Repository Structure
```text
.
â”œâ”€ artifacts/
â”‚  â”œâ”€ model_data/
â”‚  â””â”€ features.json
â”œâ”€ data/
â”‚  â”œâ”€ raw/
â”‚  â””â”€ processed/
â”œâ”€ notebooks/
â”‚  â”œâ”€ 01_EDA_feature_engineering.ipynb
â”‚  â”œâ”€ 02_modelling_optuna.ipynb
â”‚  â””â”€ 03_check_pipelines_performance.ipynb
â”œâ”€ src/
â”‚  â”œâ”€ custom_pipeline.py
â”‚  â”œâ”€ train_custom.py
â”‚  â””â”€ predict.py
â”œâ”€ app.py
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ .dockerignore
â””â”€ .gitignore
Data folders:

data/raw â€” original raw dataset

data/processed â€” processed dataset after feature engineering

Artifacts:

artifacts/ â€” everything needed for reproducibility: feature metadata, best params, thresholds, etc.

ğŸ“Š Results
> Metrics are reported using a stratified train/test split and validated with Stratified K-Fold cross-validation (via `cross_val_predict`) to preserve class distribution and reduce variance.

Pipeline Comparison
Full custom pipeline (raw â†’ processed â†’ model):

accuracy: 0.9057

precision: 0.8686

recall: 0.8889

f1: 0.8786

Standard pipeline (processed + standard preprocessing):

accuracy: 0.8945

precision: 0.8543

recall: 0.8743

f1: 0.8642

Difference (custom âˆ’ standard):

accuracy: +0.0112

precision: +0.0143

recall: +0.0146

f1: +0.0145

âœ… Conclusion: the full custom pipeline performs better, so it is kept as the final solution.

ğŸ§  How It Works (Step-by-Step)
1) EDA + Feature Engineering + Feature Selection
Started with exploratory analysis to understand:

numeric/categorical distributions

separability between Survived=1 and Survived=0

Then performed deeper statistical checks (weak signal â†’ iterate back into feature engineering):

Categorical checks

Chi-square test

Survival rate comparison

Numerical checks

Mannâ€“Whitney U (robust to non-normality and outliers)

Welchâ€™s t-test (different variances)

Cohenâ€™s d (effect size)

Next: correlation filtering to remove redundant information:

Pearson / Spearman for numeric features

CramÃ©râ€™s V for categorical features

Final selection combined a Logistic Regression baseline + ROC-AUC:

Greedy selection (add features one by one and keep only those improving the score)

L1 regularization (Lasso)

Permutation importance (important features reduce the metric when permuted; noisy features â‰ˆ 0 or negative)

Outputs:

processed dataset â†’ data/processed/

feature metadata â†’ artifacts/features.json

2) Modeling + Optuna (XGBoost)
tested multiple models and selected XGBoost

ran a sanity check: shuffle target â†’ expected ROC-AUC â‰ˆ 0.5

if it stays high, it may indicate leakage/bug/overfitting

Optuna tuning â†’ saved best parameters and tuning results

3) Full Custom Pipeline (raw â†’ processed â†’ model)
The â€œstandardâ€ approach trained only on the already processed dataset.
This project also includes a full custom pipeline that:

takes raw input

performs feature engineering inside the pipeline

trains the model using the best Optuna parameters

Files:

src/custom_pipeline.py â€” pipeline assembly

src/train_custom.py â€” training the full pipeline

notebooks/03_check_pipelines_performance.ipynb â€” pipeline comparison

â–¶ï¸ Run Locally
Install dependencies
pip install -r requirements.txt
# dev dependencies (optional)
pip install -r requirements-dev.txt
Run Streamlit
streamlit run app.py
ğŸ³ Run with Docker
Build
docker build -t titanic-streamlit .
Run
docker run --rm -p 8501:8501 titanic-streamlit
Or with Docker Compose
docker compose up --build
Open:

http://localhost:8501

ğŸ“¦ Artifacts
Stored in artifacts/model_data/:

best_params.json â€” best Optuna hyperparameters

best_score.json â€” best objective score (e.g., best_roc_auc_value)

threshold.json â€” threshold_f1 (threshold that maximizes F1; not fixed at 0.5)

Also:

artifacts/features.json â€” final selected features + metadata

ğŸ›£ Roadmap
âœ… Add Streamlit demo link â€” done

ğŸ“„ License
MIT â€” see LICENSE