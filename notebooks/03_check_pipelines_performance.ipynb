{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ae5139",
   "metadata": {},
   "source": [
    "In this notebook, i will compare 2 xgb pipelines for the similarity of the result. Why? The reason is right now i have two of them where on trained on already processed data and only had the preproce and model in the pipeline while the second, was trained on raw data with custom pipelne. Why i compare them?: I just wanted to make sure that both of them is similar and i did not make a mistake in training. \n",
    "\n",
    "Note: it is important to note that 2 pipelines will not give the same result because they were trained separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be23faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a50313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(r\"C:\\Users\\User\\all_project\\projects_in_github\\titanic_project\")\n",
    "sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e885746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent\n",
    "ART_DIR = BASE_DIR / \"artifacts\" / \"model_data\"\n",
    "FULL_MODEL_PATH = ART_DIR / \"xgb_pipeline_raw.joblib\"\n",
    "MODEL_PATH = ART_DIR / \"xgb_pipeline.joblib\"\n",
    "TRAIN_RAW_PATH = BASE_DIR / \"data\" / \"raw\" / \"train_titanic.csv\"\n",
    "TRAIN_PROCESSED_PATH = BASE_DIR / \"data\" / \"processed\" / \"titanic_processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f925808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    return joblib.load(path)\n",
    "\n",
    "def predict_survival_proba(dataset: pd.DataFrame, model) -> float:\n",
    "    return model.predict_proba(dataset)[0,1]\n",
    "\n",
    "def load_datasets(path):\n",
    "    df= pd.read_csv(path)\n",
    "    X=df.drop(columns=[\"Survived\"], axis=1)\n",
    "    y=df[\"Survived\"].astype(int).values\n",
    "    return X, y\n",
    "\n",
    "def eval_at_threshold(model, X: pd.DataFrame, y: np.ndarray, thr: float) -> dict:\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    pred = (proba >= thr).astype(int)\n",
    "\n",
    "    y = y.astype(int)\n",
    "\n",
    "    tp = int(((y == 1) & (pred == 1)).sum())\n",
    "    tn = int(((y == 0) & (pred == 0)).sum())\n",
    "    fp = int(((y == 0) & (pred == 1)).sum())\n",
    "    fn = int(((y == 1) & (pred == 0)).sum())\n",
    "\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + 1e-12)\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dd9789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on raw dataset with full custom pipeline:\n",
      "  accuracy: 0.9057239057239047\n",
      "  precision: 0.868571428571426\n",
      "  recall: 0.8888888888888862\n",
      "  f1: 0.8786127167625032\n",
      "\n",
      "Performance on processed dataset with standard pipeline:\n",
      "  accuracy: 0.8945005611672268\n",
      "  precision: 0.8542857142857118\n",
      "  recall: 0.8742690058479506\n",
      "  f1: 0.86416184971048\n",
      "Differences (raw - processed):\n",
      "  accuracy: 0.01122334455667795\n",
      "  precision: 0.014285714285714235\n",
      "  recall: 0.014619883040935533\n",
      "  f1: 0.014450867052023142\n"
     ]
    }
   ],
   "source": [
    "#loading threshold\n",
    "with (ART_DIR / \"threshold.json\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    threshold = json.load(f)['threshold_f1']\n",
    "\n",
    "#loading models\n",
    "full_model = load_model(FULL_MODEL_PATH)\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "#loading datasets\n",
    "X_raw, y_raw = load_datasets(TRAIN_RAW_PATH)\n",
    "X_proc, y_proc = load_datasets(TRAIN_PROCESSED_PATH)\n",
    "\n",
    "#quick changes for raw dataset to match features\n",
    "X_raw[\"Title\"] = X_raw[\"Name\"].str.split(\",\").str[1].str.split(\".\").str[0].str.strip()\n",
    "X_raw.drop(columns=[\"Name\"], inplace=True)\n",
    "\n",
    "#analyzing performance\n",
    "print(\"Performance on raw dataset with full custom pipeline:\")\n",
    "eval_raw=eval_at_threshold(full_model, X_raw, y_raw, threshold)\n",
    "for k, v in eval_raw.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nPerformance on processed dataset with standard pipeline:\")\n",
    "eval_proc=eval_at_threshold(model, X_proc, y_proc, threshold)\n",
    "for k, v in eval_proc.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"Differences (raw - processed):\")\n",
    "for k in eval_proc:\n",
    "    diff = eval_raw[k] - eval_proc[k]\n",
    "    print(f\"  {k}: {diff}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
